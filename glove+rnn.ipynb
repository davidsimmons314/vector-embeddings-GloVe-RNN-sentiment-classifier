{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e06a93-0560-4c54-a33b-1e2159fc5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6340cecf-2320-471f-83a5-79a311ae9f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3051ea7595c640aab3ee2ba33d819526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in vocab and embeddings from GloVe\n",
    "glove_path = 'glove.6B.50d.txt'\n",
    "\n",
    "embedding_dim = 50\n",
    "glove_vocab_size = 400000\n",
    "\n",
    "glove_vocab = []\n",
    "glove_vectors = torch.empty((glove_vocab_size, embedding_dim), dtype=torch.float)\n",
    "\n",
    "with open(glove_path, 'r', encoding='UTF-8') as f:\n",
    "    for i, line in enumerate(tqdm(f.readlines())):\n",
    "        parsed_line = line.split()\n",
    "        glove_vocab += [parsed_line[0]]\n",
    "        glove_vectors[i,:] = torch.reshape(torch.tensor(list(map(float, parsed_line[1:])), dtype=torch.float), (1,embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55c39f6-78a3-4f45-b357-36894e891548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c8d1a08a7d4b5d84d8c38e56708018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dedf33ef8f44128a7b9eeff95fbe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cdf144027f47c49e4437ecfd1ecf4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = ['<pad>'] + glove_vocab\n",
    "padding_index = 0\n",
    "\n",
    "vocab_dict = {}\n",
    "for idx in range(len(vocab)):\n",
    "    vocab_dict[vocab[idx]] = idx\n",
    "    \n",
    "# read in train filepath\n",
    "train_dir = 'review_polarity/txt_sentoken/train'\n",
    "train_data_unshuffled = []\n",
    "\n",
    "pos_dir = train_dir + '/pos'\n",
    "for filename in tqdm(os.listdir(pos_dir)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = pos_dir + '/' + filename\n",
    "        train_data_unshuffled += [(filepath, 1)]\n",
    "\n",
    "neg_dir = train_dir + '/neg'\n",
    "for filename in tqdm(os.listdir(neg_dir)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = neg_dir + '/' + filename\n",
    "        train_data_unshuffled += [(filepath, 0)]\n",
    "        \n",
    "train_data = random.sample(train_data_unshuffled, len(train_data_unshuffled))\n",
    "train_size = len(train_data)\n",
    "\n",
    "train_vocab = []\n",
    "for filename, _ in tqdm(train_data):\n",
    "    with open(filename, 'r', encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parsed_line = line.split()\n",
    "            for token in parsed_line:\n",
    "                if vocab_dict.get(token) == None:\n",
    "                    token_split = re.split('([^a-zA-Z0-9])', token)\n",
    "                    for sub_token in token_split:\n",
    "                        if vocab_dict.get(sub_token) == None:\n",
    "                            if sub_token != '':\n",
    "                                vocab_dict[sub_token] = len(vocab_dict)\n",
    "                                vocab += [sub_token]\n",
    "                                train_vocab += [sub_token]\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b89cb45-b59f-4757-9e05-a0b17718d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('review_polarity/txt_sentoken/train/pos/cv002_15918.txt', 1), ('review_polarity/txt_sentoken/train/neg/cv003_12683.txt', 0), ('review_polarity/txt_sentoken/train/pos/cv001_18431.txt', 1), ('review_polarity/txt_sentoken/train/pos/cv003_11664.txt', 1), ('review_polarity/txt_sentoken/train/neg/cv001_19502.txt', 0), ('review_polarity/txt_sentoken/train/neg/cv002_17424.txt', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550596d3-7aab-4148-8db1-4d5265ff3343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('review_polarity/txt_sentoken/train/pos/cv003_11664.txt', 1), ('review_polarity/txt_sentoken/train/neg/cv001_19502.txt', 0), ('review_polarity/txt_sentoken/train/pos/cv001_18431.txt', 1), ('review_polarity/txt_sentoken/train/pos/cv002_15918.txt', 1), ('review_polarity/txt_sentoken/train/neg/cv003_12683.txt', 0), ('review_polarity/txt_sentoken/train/neg/cv002_17424.txt', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(train_data_unshuffled, len(train_data_unshuffled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c53726-b175-494e-abc6-5ffe8a1e6d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfa2eebf0014387a807ec7436fa4257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in train dataset\n",
    "train_dir = 'review_polarity/txt_sentoken/train'\n",
    "\n",
    "train_file_to_idx = {}\n",
    "train_idx_to_file = np.empty(train_size, dtype=np.dtype('U100'))\n",
    "train_idx_to_label = np.empty(train_size, dtype=int)\n",
    "\n",
    "train_idx_to_vector = {}\n",
    "train_idx_to_length = np.empty(train_size, dtype=int)\n",
    "\n",
    "for idx, (filename, label) in tqdm(enumerate(train_data)):\n",
    "    train_file_to_idx[filename] = idx\n",
    "    train_idx_to_file[idx] = filename\n",
    "    train_idx_to_label[idx] = label\n",
    "    \n",
    "    vector = []\n",
    "    with open(filename, 'r', encoding='UTF-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parsed_line = line.split()\n",
    "            for token in parsed_line:\n",
    "                if vocab_dict.get(token) != None:\n",
    "                    vector += [vocab_dict[token]]\n",
    "                else:\n",
    "                    token_split = re.split('([^a-zA-Z0-9])', token)\n",
    "                    for sub_token in token_split:\n",
    "                        if vocab_dict.get(sub_token) != None and sub_token != '':\n",
    "                            vector += [vocab_dict[sub_token]]\n",
    "    train_idx_to_vector[idx] = vector\n",
    "    train_idx_to_length[idx] = len(vector)\n",
    "    \n",
    "train_idx = list(range(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff311ce-b04b-4bfd-9f7b-88afb8a4986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(train_file_to_idx)\\nprint(train_idx_to_file)\\nprint(train_idx_to_label)\\nprint(train_idx_to_vector)\\nprint(train_idx_to_length)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_file_to_idx)\n",
    "print(train_idx_to_file)\n",
    "print(train_idx_to_label)\n",
    "print(train_idx_to_vector)\n",
    "print(train_idx_to_length)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6e967a-a5d8-40ee-aa8f-c951ae2b2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_polarity/txt_sentoken/train/pos/cv002_15918.txt 1\n",
      "review_polarity/txt_sentoken/train/neg/cv003_12683.txt 0\n",
      "review_polarity/txt_sentoken/train/pos/cv001_18431.txt 1\n",
      "review_polarity/txt_sentoken/train/pos/cv003_11664.txt 1\n",
      "review_polarity/txt_sentoken/train/neg/cv001_19502.txt 0\n",
      "review_polarity/txt_sentoken/train/neg/cv002_17424.txt 0\n"
     ]
    }
   ],
   "source": [
    "for idx, (file, length, label) in enumerate(zip(train_idx_to_file, train_idx_to_length, train_idx_to_label)):\\\n",
    "    print(file, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265f1bff-06e4-4b43-9fa0-f6adc9d8d193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd0285fd12248039586749812fd42a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0, 1, 2])\n",
      "1 tensor([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "train_loader = torch.utils.data.DataLoader(train_idx, batch_size=batch_size)\n",
    "\n",
    "for index, batch_idxs in enumerate(tqdm(train_loader)):\n",
    "    print(index, batch_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af58aaf2-3bc5-41b4-92e6-47c13b22396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def batch_to_padded_tensors(batch, mode):\n",
    "    if mode == 'train':\n",
    "        idx_to_vector = train_idx_to_vector\n",
    "        idx_to_length = train_idx_to_length\n",
    "    elif mode == 'dev':\n",
    "        idx_to_vector = dev_idx_to_vector\n",
    "        idx_to_length = dev_idx_to_length\n",
    "    elif mode == 'test':\n",
    "        idx_to_vector = test_idx_to_vector\n",
    "        idx_to_length = test_idx_to_length\n",
    "    \n",
    "    size = len(batch)\n",
    "    lengths = idx_to_length[batch]\n",
    "    max_length = max(lengths)\n",
    "    #print(lengths, max_length)\n",
    "    \n",
    "    tensors = torch.zeros((size, max_length), dtype=int).long()\n",
    "    #print(tensors.shape)\n",
    "    for i, idx in enumerate(batch):\n",
    "        #print(i, idx, train_idx_to_length[idx])\n",
    "        #print(train_idx_to_vector[idx.item()])\n",
    "        tensors[i, 0:idx_to_length[idx]] = torch.LongTensor(idx_to_vector[idx.item()])\n",
    "               \n",
    "    return tensors.to(device)\n",
    "\n",
    "def batch_to_labels(batch, mode):\n",
    "    if mode == 'train':\n",
    "        idx_to_label = train_idx_to_label\n",
    "    elif mode == 'dev':\n",
    "        idx_to_label = dev_idx_to_label\n",
    "    elif mode == 'test':\n",
    "        idx_to_label = test_idx_to_label\n",
    "        \n",
    "    return torch.LongTensor(idx_to_label[batch]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c13735ad-baf7-4cf8-ae62-591b67df6a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82, 58, 37139, 406, 1340, 851, 119677, 440, 74, 21, 8932, 5, 3, 7, 461, 5, 160, 1, 320, 8, 1046, 2, 65, 40, 41, 5, 89, 16, 1785, 56, 2709, 815, 6, 5063, 1570, 2, 34, 102, 594, 1, 2492, 11, 60, 56, 607, 6, 128, 5199, 1, 2244, 3, 85, 568, 2051, 16, 792, 6, 64, 15, 37, 30, 930, 47, 24066, 5227, 7, 21, 58, 1535, 720, 24, 21, 58, 1535, 5204, 8, 1337, 303339, 4, 1, 2856, 205, 1, 2604, 2, 92, 1024, 8, 307, 1195, 17826, 25, 3, 4656, 2, 21, 1433, 99, 6, 26109, 65, 8446, 4, 220, 3227, 18081, 3, 21, 58, 1535, 11014, 17615, 6, 23, 247, 14908, 48622, 2, 37, 5, 3863, 192, 31632, 3, 35, 3203, 2, 198, 19613, 13, 12533, 15, 3, 35, 64, 391, 31, 646, 69, 74, 1, 7029, 6, 12533, 13, 908, 1, 1006, 162, 20, 144, 20, 21, 261, 2, 114, 42, 3961, 13073, 1, 772, 3513, 275, 17811, 21066, 2, 32654, 7, 2557, 3, 21, 163652, 58, 2160, 34, 52, 1, 8045, 2, 114, 151, 1589, 36, 61363, 22, 1, 216, 788, 3, 42, 6015, 58, 2160, 1690, 554, 8155, 67, 103, 42, 5573, 101, 182, 60, 82, 58, 37139, 406, 1340, 2, 35, 128, 379, 2, 15, 13, 589, 482, 189, 84, 82, 118, 646, 101, 182, 2, 739, 152, 996, 21, 189, 379, 2, 1, 13304, 15, 20, 55269, 20, 40, 327, 3, 1615, 17811, 1382, 1985, 2107, 2, 1, 64190, 58461, 1779, 4, 8, 6101, 540, 2854, 6, 18720, 3513, 1382, 11125, 13355, 2, 1, 152, 57, 64190, 58461, 23246, 4, 8, 118489, 272, 58, 1535, 540, 2856, 176, 2, 7, 8, 3083, 12124, 2, 1, 2856, 205, 1, 2604, 3, 2107, 6, 13355, 721, 400, 4505, 3020, 114, 1, 51, 2107, 1146, 1613, 15, 876, 61, 249, 532, 1, 1964, 26, 1, 358, 259, 3, 334, 89, 40, 347, 2, 40, 33, 412, 7, 836, 18, 237, 69, 75, 1, 926, 2, 92, 2243, 166, 2199, 1, 69, 900, 58, 1535, 1447, 3005, 3, 1, 1034, 4, 1, 524, 75361, 58, 2160, 482, 114, 65, 21, 261, 15, 1368, 20, 8, 6576, 9430, 11, 1, 56, 1570, 5, 594, 1, 2492, 3, 1086, 2, 64, 33, 78, 20207, 4002, 48686, 2, 35, 40, 65, 4356, 7, 5460, 5, 1, 18340, 80036, 4, 1, 445, 1484, 3, 65, 4, 38, 2, 4, 747, 2, 2391, 61, 5, 1, 12671, 14800, 3, 35, 20, 19698, 20, 1, 1946, 15, 2, 21, 58, 1535, 101, 16059, 14034, 6, 244440, 13, 42, 2566, 131, 1006, 7, 1, 1453, 63, 2434, 8, 1501, 1, 24627, 20, 182, 5745, 5974, 20, 38, 154, 261, 3, 62, 3513, 10889, 1, 1447, 3005, 4, 72, 1293, 836, 2, 42, 16, 2704, 18, 126, 2, 11, 1493, 4, 8, 440, 1389, 2, 9812, 13, 11, 1, 59, 80, 65, 63, 2, 42, 1404, 219, 1, 2249, 8782, 3]\n"
     ]
    }
   ],
   "source": [
    " print(train_idx_to_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bad10e-017e-4c8f-b585-ef630a9b6406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9dcf366b47462c911d84de865dcbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0, 1, 2])\n",
      "tensor([[   82,    58, 37139,  ...,     0,     0,     0],\n",
      "        [    9,  6404,    11,  ...,     0,     0,     0],\n",
      "        [  360,   115,     6,  ...,     8,  6713,     3]], device='cuda:0')\n",
      "tensor([1, 0, 1], device='cuda:0')\n",
      "1 tensor([3, 4, 5])\n",
      "tensor([[    9, 20309,     9,  ...,     4,  2291,     3],\n",
      "        [    1,  1752, 32308,  ...,     0,     0,     0],\n",
      "        [   21,    15,  2460,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([1, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for index, batch in enumerate(tqdm(train_loader)):\n",
    "    print(index, batch)\n",
    "    batch_tensors = batch_to_padded_tensors(batch, 'train')\n",
    "    batch_labels= batch_to_labels(batch, 'train')\n",
    "    \n",
    "    print(batch_tensors)\n",
    "    print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ebaa77f-aaa4-4b15-9b0d-9317422fa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_to_tensor(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        parsed_text = text.split()\n",
    "        #print(parsed_text)\n",
    "        \n",
    "        vector = []\n",
    "        for i in range(len(parsed_text)):\n",
    "            if parsed_text[i] in vocab:\n",
    "                vector += [vocab_dict[parsed_text[i]]]\n",
    "\n",
    "    return torch.LongTensor(vector).to(device)\n",
    "\n",
    "def filename_to_vector(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        parsed_text = text.split()\n",
    "        #print(parsed_text)\n",
    "        \n",
    "        vector = []\n",
    "        for i in range(len(parsed_text)):\n",
    "            if parsed_text[i] in vocab:\n",
    "                vector += [vocab_dict[parsed_text[i]]]\n",
    "    return vector, len(vector)\n",
    "\n",
    "def filenames_to_vectors(filenames):\n",
    "    size = len(filenames)\n",
    "    max_length = 0\n",
    "    vectors = []\n",
    "    for filename in filenames:\n",
    "        vector = filename_to_vector(filename)\n",
    "        if len(vector) > max_length:\n",
    "            max_length = len(vector)\n",
    "        vectors += [filename_to_vector(filename)]\n",
    "        \n",
    "    return vectors, max_length\n",
    "\n",
    "def filenames_to_padded_tensors(filenames):\n",
    "    size = len(filenames)\n",
    "    vectors, max_length = filenames_to_vectors(filenames)\n",
    "    padded_vectors = []\n",
    "    for vector in vectors:\n",
    "        for i in range(len(vector), max_length):\n",
    "            vector += [padding_index]\n",
    "        padded_vectors += [vector]\n",
    "    \n",
    "    return torch.LongTensor(padded_vectors).to(device)\n",
    "\n",
    "def batch_to_padded_tensors(batch_vectors, max_length):\n",
    "    size = len(batch_vectors)\n",
    "    padded_vectors = []\n",
    "    for vector in batch_vectors:\n",
    "        for i in range(len(vector), max_length):\n",
    "            vector += [padding_index]\n",
    "        padded_vectors += [vector]\n",
    "    \n",
    "    return torch.LongTensor(padded_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b78d36e5-11d3-4709-9303-1a0675210eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e5c051cf2a4652b600fdf022b345db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''train_data_vectors = {}\n",
    "\n",
    "for filename, _ in tqdm(train_data):\n",
    "    file_vector = filename_to_vector(filename)\n",
    "    train_data_vectors[filename] = file_vector'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16583a2-c558-439a-a1e9-29a1f26ee84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d865406b93e54bc2addbf038062351b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''dev_data_vectors = {}\n",
    "\n",
    "for filename, _ in tqdm(dev_data):\n",
    "    file_vector = filename_to_vector(filename)\n",
    "    dev_data_vectors[filename] = file_vector'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f3e510-f798-4377-9b01-1d0c7de4cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "hidden_size = 32\n",
    "num_labels = 2\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d7967-9ef6-4598-b4fa-22b6000f8d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d5305b-9e8f-45f6-8b8a-f7704b31716d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cwd = os.getcwd()\\ntrain_dir = cwd + '/review_polarity/txt_sentoken'\\npath1 = train_dir + '/example1.txt'\\npath2 = train_dir + '/example2.txt'\\npaths = [path1, path2]\\nprint(filename_to_vector(path1))\\nprint(filename_to_vector(path2))\\nprint(filenames_to_vectors(paths))\\nprint(filenames_to_padded_tensors(paths))\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cwd = os.getcwd()\n",
    "train_dir = cwd + '/review_polarity/txt_sentoken'\n",
    "path1 = train_dir + '/example1.txt'\n",
    "path2 = train_dir + '/example2.txt'\n",
    "paths = [path1, path2]\n",
    "print(filename_to_vector(path1))\n",
    "print(filename_to_vector(path2))\n",
    "print(filenames_to_vectors(paths))\n",
    "print(filenames_to_padded_tensors(paths))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c776b88-bf75-4a60-9b86-7fb2faab63af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_index)\n",
    "        \n",
    "        # initialize embeddings with glove embeddings\n",
    "        with torch.no_grad():\n",
    "            for i in range(glove_vocab_size):\n",
    "                self.embedding.weight[i] = glove_vectors[i]\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.hidden_to_label = nn.Linear(hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, document_tensors):\n",
    "        \"\"\"\n",
    "        document_vectors : batch_size x sequence_length\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length = document_tensors.shape\n",
    "        hidden_0 = torch.zeros((1, batch_size, self.hidden_size),dtype=torch.float).to(device)\n",
    "        \n",
    "        #print(document_vectors.shape)\n",
    "        \n",
    "        document_embeddings = self.embedding(document_tensors)\n",
    "        \n",
    "        #print(document_embeddings.shape)\n",
    "        rnn_out, _ = self.rnn(document_embeddings.view(batch_size, sequence_length, embedding_dim), hidden_0)\n",
    "        pooling_out = self.pooling(rnn_out.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        score = self.hidden_to_label(pooling_out).view(batch_size, -1)\n",
    "        \n",
    "        return F.log_softmax(score, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83baf12e-8146-4e8f-a2a8-41eb2be17bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Classifier(hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c47adf56-ba5b-4477-81fa-1da5a1c73753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, mode):\n",
    "    true_positive_count = 0\n",
    "    false_positive_count = 0\n",
    "    true_negative_count = 0\n",
    "    false_negative_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, batch in enumerate(tqdm(data_loader)):\n",
    "            print(index, batch)\n",
    "            current_batch_size = len(batch)\n",
    "            \n",
    "            batch_tensors = batch_to_padded_tensors(batch, mode)\n",
    "            batch_labels = batch_to_labels(batch, mode)\n",
    "            print(batch_tensors)\n",
    "            print(batch_labels)\n",
    "\n",
    "            log_probs = model(batch_tensors)\n",
    "            \n",
    "            print(log_probs)\n",
    "            \n",
    "            predicted_labels = torch.argmax(log_probs, 1)\n",
    "            print(predicted_labels)\n",
    "\n",
    "            for i in range(current_batch_size):\n",
    "                if batch_labels[i] == 1 and predicted_labels[i] == 1:\n",
    "                    true_positive_count += 1\n",
    "                    #print('true pos', true_positive_count)\n",
    "                elif batch_labels[i] == 0 and predicted_labels[i] == 1:\n",
    "                    false_positive_count += 1\n",
    "                    #print('false pos', false_positive_count)\n",
    "                elif batch_labels[i] == 0 and predicted_labels[i] == 0:\n",
    "                    true_negative_count += 1\n",
    "                    #print('true neg', true_negative_count)\n",
    "                elif batch_labels[i] == 1 and predicted_labels[i] == 0:\n",
    "                    false_negative_count += 1\n",
    "                    #print('false neg', false_negative_count)\n",
    "\n",
    "    print(true_positive_count)\n",
    "    print(false_positive_count)\n",
    "    print(true_negative_count)\n",
    "    print(false_negative_count)\n",
    "\n",
    "    accuracy = (true_positive_count + true_negative_count) / (true_positive_count + false_positive_count + true_negative_count + false_negative_count)\n",
    "    \n",
    "    if true_positive_count + false_positive_count != 0:\n",
    "        precision = true_positive_count / (true_positive_count + false_positive_count)\n",
    "    else: \n",
    "        precision = 0\n",
    "        \n",
    "    recall = true_positive_count / (true_positive_count + false_negative_count)\n",
    "\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    #print('accuracy = {val:.3f}'.format(val = accuracy))\n",
    "    #print('F1 = {val:.3f}'.format(val = F1))\n",
    "    \n",
    "    return accuracy, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "122555a3-dd62-406c-9432-b6089051383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_train_accuracy = [0] * num_epochs\n",
    "    epoch_train_F1 = [0] * num_epochs\n",
    "    \n",
    "    epoch_dev_accuracy = [0] * num_epochs\n",
    "    epoch_dev_F1 = [0] * num_epochs\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('epoch', epoch)\n",
    "        for index, batch in enumerate(tqdm(train_loader)):\n",
    "            print(index, batch)\n",
    "            current_batch_size = len(batch)\n",
    "            \n",
    "            batch_tensors = batch_to_padded_tensors(batch, 'train')\n",
    "            batch_labels = batch_to_labels(batch, 'train')\n",
    "            print(batch_tensors)\n",
    "            print(batch_labels)\n",
    "    \n",
    "\n",
    "            #document_tensors = filenames_to_padded_tensors(batch_files)\n",
    "            #labels = labels.to(device)\n",
    "            #print(document_tensors)\n",
    "            #print(labels)\n",
    "            \n",
    "            log_probs = model(batch_tensors)\n",
    "            \n",
    "            print(log_probs)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss = F.nll_loss(log_probs, batch_labels)\n",
    "            \n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print('train')\n",
    "        epoch_train_accuracy[epoch], epoch_train_F1[epoch] = evaluate(model, train_loader, 'train')\n",
    "        print(epoch_train_accuracy[epoch], epoch_train_F1[epoch] )\n",
    "        #print('dev')\n",
    "        #epoch_dev_accuracy[epoch], epoch_dev_F1[epoch] = evaluate(model, dev_loader, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eed3b302-1d24-4993-81ee-288ae26d5ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c593d9057a40049be085ee80636190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0, 1, 2])\n",
      "tensor([[   82,    58, 37139,  ...,     0,     0,     0],\n",
      "        [   21,    15,  2460,  ...,    65,  1139,     3],\n",
      "        [    1,  1752, 32308,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([1, 0, 0], device='cuda:0')\n",
      "tensor([[-1.3651, -0.2948],\n",
      "        [-1.2103, -0.3540],\n",
      "        [-1.0695, -0.4204]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(0.8582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 tensor([3, 4, 5])\n",
      "tensor([[    9,  6404,    11,  ...,     0,     0,     0],\n",
      "        [  360,   115,     6,  ...,     0,     0,     0],\n",
      "        [    9, 20309,     9,  ...,     4,  2291,     3]], device='cuda:0')\n",
      "tensor([0, 1, 1], device='cuda:0')\n",
      "tensor([[-0.3360, -1.2541],\n",
      "        [-0.4240, -1.0626],\n",
      "        [-0.4013, -1.1069]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(0.8352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d77bd6ce2dd483f862d6ea906fde810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0, 1, 2])\n",
      "tensor([[   82,    58, 37139,  ...,     0,     0,     0],\n",
      "        [   21,    15,  2460,  ...,    65,  1139,     3],\n",
      "        [    1,  1752, 32308,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([1, 0, 0], device='cuda:0')\n",
      "tensor([[-1.3566, -0.2978],\n",
      "        [-1.1300, -0.3902],\n",
      "        [-0.9811, -0.4699]], device='cuda:0')\n",
      "tensor([1, 1, 1], device='cuda:0')\n",
      "1 tensor([3, 4, 5])\n",
      "tensor([[    9,  6404,    11,  ...,     0,     0,     0],\n",
      "        [  360,   115,     6,  ...,     0,     0,     0],\n",
      "        [    9, 20309,     9,  ...,     4,  2291,     3]], device='cuda:0')\n",
      "tensor([0, 1, 1], device='cuda:0')\n",
      "tensor([[-1.2033, -0.3570],\n",
      "        [-1.4420, -0.2698],\n",
      "        [-1.4271, -0.2745]], device='cuda:0')\n",
      "tensor([1, 1, 1], device='cuda:0')\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0.5 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "18197300-69b7-474a-b444-5868075dc56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3cc555653145aa875d707f7b67aeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0, 1, 2])\n",
      "tensor([[   82,    58, 37139,  ...,     0,     0,     0],\n",
      "        [   21,    15,  2460,  ...,    65,  1139,     3],\n",
      "        [    1,  1752, 32308,  ...,     0,     0,     0]], device='cuda:0')\n",
      "tensor([1, 0, 0], device='cuda:0')\n",
      "tensor([[-1.3566, -0.2978],\n",
      "        [-1.1300, -0.3902],\n",
      "        [-0.9811, -0.4699]], device='cuda:0')\n",
      "tensor([1, 1, 1], device='cuda:0')\n",
      "1 tensor([3, 4, 5])\n",
      "tensor([[    9,  6404,    11,  ...,     0,     0,     0],\n",
      "        [  360,   115,     6,  ...,     0,     0,     0],\n",
      "        [    9, 20309,     9,  ...,     4,  2291,     3]], device='cuda:0')\n",
      "tensor([0, 1, 1], device='cuda:0')\n",
      "tensor([[-1.2033, -0.3570],\n",
      "        [-1.4420, -0.2698],\n",
      "        [-1.4271, -0.2745]], device='cuda:0')\n",
      "tensor([1, 1, 1], device='cuda:0')\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.6666666666666666)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, train_loader, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e4cbe4c-004f-4c03-b685-8c064119e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95b1318a-b9b7-472e-b4b0-633d2e4f7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array(L, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "babbbd9a-7495-42c3-8b75-9b7148331dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e6991d5-5e47-476c-a351-5714fd3ed410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'c'], dtype='<U1')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1c1ea-b369-4034-a64f-2e6ef32b3f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7e2c6-94b3-41f9-bb83-6118130e59c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
